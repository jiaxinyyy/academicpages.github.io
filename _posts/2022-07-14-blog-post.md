---
title: 'NLP Learning'
date: 2022-07-14
permalink: /posts/202/07/NLP-Learning/
tags:
  - NLP
---

This is a record of my NLP learning.

#### Kaggle Project: NLP with Disaster Tweets
- Basic info\
project goal: build a model that predicts if a tweet is about disaster or not\
input: a dataset of 10,000 tweets that were hand classified (tabular data with columns “id”, “keyword”, “location”, “text”, “target”)\
output: a table with columns “id” and “target” (label 0 or 1)

- EDA\
  missing values in “keyword” and “location”\
  fill in with “no_keyword” or “no_location”\
  the values of “location” are chosen by users, and they are dirty to use\
  use values of “keyword” as the feature\
  use sns.countplot() to show the count of targets of each keyword\
  meta features (features beyond given features)\
  use sns.distplot() to show if each feature has very different distributions for disaster and non-disaster tweets

- N-grams (pre-processing)\
  divide into N-grams\
  compare the counts of unigrams/bigrams/trigrams exist in disaster/non-disaster tweets

- Embeddings (featurize)\
  use pre-trained embedding models: GloVe, FastText\
  calculate the text coverage in both models
  
- Text cleaning\
  use re.sub() to replace matched strings in tweets

- Cross-validation\
  use StratifiedKFold()

- Model\
  metric (accuracy, precision, recall, F1 score)
- Bert layer



